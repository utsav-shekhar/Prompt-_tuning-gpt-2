# -*- coding: utf-8 -*-
"""ass-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10DBYabAlphxHo4t7tVoQ0be8aW-eSTZW
"""

!pip install transformers
!pip install datasets

from tqdm import tqdm
import pandas as pd
from sklearn.model_selection import train_test_split

"""**Summariser**"""

import torch
from torch.nn import Embedding
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

# Load GPT-2 model and tokenizer
CACHE_DIR = "/ssd_scratch/cogsci/utsav12/.cache"
HF_TOKEN="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"
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name, token=HF_TOKEN,cache_dir= CACHE_DIR)
tokenizer = GPT2Tokenizer.from_pretrained(model_name,token=HF_TOKEN,cache_dir= CACHE_DIR)

class SoftPromptEmbedding(torch.nn.Module):
    def __init__(self, num_prompts, embedding_size=768):
        super(SoftPromptEmbedding, self).__init__()
        self.embedding = Embedding(num_prompts, embedding_size)

    def forward(self, prompt_ids):
        return self.embedding(prompt_ids)

class GPT2WithSoftPrompt(torch.nn.Module):
    def __init__(self, base_model, soft_prompt_embedding):
        super(GPT2WithSoftPrompt, self).__init__()
        self.base_model = base_model
        self.soft_prompt_embedding = soft_prompt_embedding

    def forward(self, input_ids, prompt_ids):
        prompt_embeddings = self.soft_prompt_embedding(prompt_ids)
        input_embeddings = self.base_model.transformer.wte(input_ids)
        embeddings = torch.cat([prompt_embeddings, input_embeddings], dim=0)

        return self.base_model(inputs_embeds=embeddings)

from datasets import load_dataset

print("Loading dataset")
train_file = "/content/summariser.csv"

# Read only the first 1000 rows of the data
df = pd.read_csv(train_file, nrows=100)
df = df.dropna()

# Convert df to a dictionary with article and highlights
train_data = {"article": df["article"].tolist(), "highlights": df["highlights"].tolist()}
val_data = train_data

# Rest of your code...
from datasets import load_dataset

print("Loading dataset")
train_file = "/content/summariser.csv"

# Read only the first 1000 rows of the data
df = pd.read_csv(train_file, nrows=1000)
df = df.dropna()

# Convert df to a dictionary with article and highlights
train_data = {"article": df["article"].tolist(), "highlights": df["highlights"].tolist()}
val_data = train_data

# Rest of your code...

from transformers import GPT2Tokenizer
from tqdm import tqdm
import torch

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
prompt_token = "SUMMARIZE"
prompt_id = tokenizer.encode(prompt_token)
prompt_id = torch.tensor(prompt_id, dtype=torch.long)
num_prompts = len(prompt_id)
prompt_id = torch.arange(num_prompts)

def preprocess_data(data):
    # Tokenize and prepend the prompt token to the articles
    tokenized_articles = []
    tokenized_summaries = []

    for article, summary in tqdm(zip(data["article"], data["highlights"]), total=len(data["article"]), desc="Tokenizing"):
        article_tokens = tokenizer.encode(article, truncation=True, max_length=1020)
        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=150)

        max_length_article = 1024 - num_prompts
        max_length_summary = 1024
        padded_article = article_tokens + [tokenizer.pad_token_id] * (max_length_article - len(article_tokens))
        padded_summary = summary_tokens + [tokenizer.pad_token_id] * (max_length_summary - len(summary_tokens))

        tokenized_articles.append(padded_article)
        tokenized_summaries.append(padded_summary)

    return torch.tensor(tokenized_articles), torch.tensor(tokenized_summaries)

train_articles, train_summaries = preprocess_data(train_data)
val_articles, val_summaries = preprocess_data(val_data)

print(train_articles)

model = GPT2LMHeadModel.from_pretrained("gpt2",cache_dir= CACHE_DIR)
soft_prompt_embedding = SoftPromptEmbedding(num_prompts)
device = torch.device("cuda")
model_with_prompt = GPT2WithSoftPrompt(model, soft_prompt_embedding).to(device)

# Hyperparameters
BATCH_SIZE = 2
EPOCHS = 1
GRADIENT_ACCUMULATION_STEPS = 4
GRADIENT_CLIP_NORM = 1.0
EARLY_STOPPING_PATIENCE = 2
prompt_id = prompt_id.to(device)
# Import cross_entropy_loss
from torch.nn import CrossEntropyLoss

# ...

# ...

def fine_tune_on_summarization(model_with_prompt, train_articles, train_summaries, val_articles, val_summaries, save_path="fine_tuned_model"):
    optimizer = torch.optim.Adam(model_with_prompt.soft_prompt_embedding.parameters())

    best_val_loss = float('inf')
    no_improvement_epochs = 0

    for epoch in range(EPOCHS):
        model_with_prompt.train()

        # Gradient accumulation initialization
        optimizer.zero_grad()
        accumulated_loss = 0
        loss = 0
        # Use tqdm for progress bar
        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f"Epoch {epoch + 1}/{EPOCHS}", unit="batch") as progress:
            for idx, (article, summary) in progress:
                input_ids = torch.tensor(article).to(device)
                labels = torch.tensor(summary).to(device)
                outputs = model_with_prompt(input_ids, prompt_id)

                ignore_index = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -100
                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)

                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset
                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_articles) - 1:
                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()
                    torch.nn.utils.clip_grad_norm_(model_with_prompt.parameters(), GRADIENT_CLIP_NORM)
                    optimizer.step()
                    optimizer.zero_grad()
                    loss = 0

        # Validation
        # model_with_prompt.eval()
        # total_val_loss = 0
        # with torch.no_grad():
        #     for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc="Validation", unit="batch"):
        #         input_ids = torch.tensor(article, dtype=torch.long).clone().detach()
        #         labels = torch.tensor(summary, dtype=torch.long).clone().detach()
        #         outputs = model_with_prompt(input_ids, prompt_id)

        #         ignore_index = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -100
        #         val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)
        #         total_val_loss += val_loss.item()

        # avg_val_loss = total_val_loss / len(val_articles)

        # # Early stopping
        # if avg_val_loss < best_val_loss:
        #     best_val_loss = avg_val_loss
        #     no_improvement_epochs = 0
        # else:
        #     no_improvement_epochs += 1
        #     if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:
        #         print(f"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.")
        #         break

    # Save the fine-tuned model using PyTorch save
    save_path = save_path + "_epoch_" + str(epoch + 1) + ".pt"
    torch.save(model_with_prompt.state_dict(), save_path)
    print(f"Fine-tuned model saved at {save_path}")

    return model_with_prompt

fine_tuned_model = fine_tune_on_summarization(model_with_prompt, train_articles, train_summaries, val_articles, val_summaries)

print(input_ids.size())

print(model_with_prompt)

def generate_summary(model, prompt, max_length=150):
    input_text = f"{prompt} QUESTION: SUMMARIZE"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)  # Move input to GPU

    with torch.no_grad():
        # Use the underlying GPT-2 model for generation
        output = model.base_model.generate(input_ids.to(device), max_length=max_length, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)

    summary = tokenizer.decode(output[0], skip_special_tokens=True)
    return summary

# Example usage
paragraph = "GPT-2 is a powerful language model developed by OpenAI. It has 1.5 billion parameters and is capable of generating coherent and contextually relevant text. The model has been widely used for various natural language processing tasks, including text completion, text generation, and summarization."

# Move the model to GPU if not already on GPU
fine_tuned_model.to(device)

generated_summary = generate_summary(fine_tuned_model, paragraph)
print("Original Paragraph:")
print(paragraph)
print("\nGenerated Summary:")
print(generated_summary)

import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import pandas as pd
from tqdm import tqdm
from torch.nn import CrossEntropyLoss

# Constants
MODEL_NAME = "gpt2"
BATCH_SIZE = 1
EPOCHS = 1
PROMPT_TOKEN = "Summarize the following sentence :"
MAX_LEN = 1024

# Soft Prompt Vocabulary
soft_prompt_vocab = ["Summarize", "the", "following", "sentence", ":"]  # Define your custom vocabulary here
# Create a word2idx dictionary for the soft prompt vocabulary
soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}

num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])
prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])

# Data Loading and Preprocessing
class DataLoaderAndPreprocessor:
    def __init__(self, file_path, num_prompts, fraction=0.0001):
        self.file_path = file_path
        self.num_prompts = num_prompts
        self.fraction = fraction

    def load_data(self):
        df = pd.read_csv(self.file_path)
        df = df.dropna().sample(frac=self.fraction)

        return df

    def preprocess_data(self, article, summary):
        # Your existing preprocessing logic here
        pass


# Model Architecture
class GPT2WithSoftPrompt(torch.nn.Module):
    def __init__(self, model_name, num_prompts, embedding_size=768):
        super(GPT2WithSoftPrompt, self).__init__()
        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)
        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)

    def forward(self, input_ids, prompt_ids):
        prompt_embeddings = self.soft_prompt(prompt_ids)
        base_embeddings = self.gpt2.transformer.wte(input_ids)
        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)
        outputs = self.gpt2(inputs_embeds=embeddings)
        return outputs


# Trainer
class Trainer:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def train(self, train_data, validation_data, test_data):
        # Your training logic here
        pass

    def evaluate(self, tokenized_articles, tokenized_summaries):
        # Your evaluation logic here
        pass


# Usage
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)

data_loader_and_preprocessor = DataLoaderAndPreprocessor("/content/summariser.csv", num_prompts)
df_train = data_loader_and_preprocessor.load_data()
tokenized_articles_train, tokenized_summaries_train = data_loader_and_preprocessor.preprocess_data(
    df_train["article"], df_train["highlights"]
)

# Model Initialization
model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to("cpu")

# Trainer Initialization
trainer = Trainer(model, tokenizer, device="cpu")

# Training
trainer.train(
    train_data=(tokenized_articles_train, tokenized_summaries_train),
    validation_data=None,  # Add validation data here if available
    test_data=(tokenized_articles_train, tokenized_summaries_train),
)

# Evaluation
trainer.evaluate(tokenized_articles_train, tokenized_summaries_train)

"""**Question answering**"""

import torch
from torch.nn import Embedding
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

# Load GPT-2 model and tokenizer
CACHE_DIR = "/ssd_scratch/cogsci/utsav12/.cache"
HF_TOKEN="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"
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name, token=HF_TOKEN,cache_dir="/ssd_scratch/cogsci/utsav12/.cache")
tokenizer = GPT2Tokenizer.from_pretrained(model_name,token=HF_TOKEN,cache_dir="/ssd_scratch/cogsci/utsav12/.cache")

print(model)

import pandas as pd
import json

# Load JSON data from file
with open('/content/train-v2.0.json', 'r') as json_file:
    data = json.load(json_file)

# Normalize nested JSON structures
df = pd.json_normalize(data)

# Display the DataFrame
print(df)

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

# Assuming you have already trained a GPT-2 model for summarization
# Load GPT-2 model and tokenizer
CACHE_DIR = "/ssd_scratch/cogsci/utsav12/.cache"
HF_TOKEN = "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"
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name, token=HF_TOKEN, cache_dir=CACHE_DIR)
tokenizer = GPT2Tokenizer.from_pretrained(model_name, token=HF_TOKEN, cache_dir=CACHE_DIR)

# Function to perform question answering using GPT-2
def answer_question(context, question, max_length=200):
    input_text = f"{context} QUESTION: {question}"

    # Tokenize input text
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Generate answer
    with torch.no_grad():
        output = model.generate(input_ids, max_length=max_length, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)

    # Decode and return the generated answer
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# Example usage
context = "GPT-2 is a powerful language model."
question = "What is GPT-2?"

result = answer_question(context, question)
print("Answer:", result)

def prepare_data(data, percentage=1.0):
    examples = []

    # Sample a percentage of data
    sampled_data = random.sample(data, int(len(data) * percentage))

    for article in tqdm(sampled_data, desc="Processing data"):
        for paragraph in article["paragraphs"]:
            context = paragraph["context"]
            for qa in paragraph["qas"]:
                question = qa["question"]

                # Check if there are answers
                if not qa["answers"]:
                    continue

                answer_text = qa["answers"][0]["text"]
                start_char = qa["answers"][0]["answer_start"]
                end_char = start_char + len(answer_text)

                # Tokenize
                inputs = tokenizer(question, context, return_tensors="pt", max_length=512, truncation=True)

                # Find start and end positions
                start_positions = tokenizer(context[:start_char], return_tensors="pt")["input_ids"].shape[1]
                end_positions = tokenizer(context[:end_char], return_tensors="pt")["input_ids"].shape[1]

                examples.append(
                    {
                        "input_ids": inputs["input_ids"].squeeze(),
                        "attention_mask": inputs["attention_mask"].squeeze(),
                        "start_positions": start_positions,
                        "end_positions": end_positions,
                    }
                )

    return examples

# Set the percentage to 10%
sample_percentage = 0.1

# Use only 10% of the training data
train_examples = prepare_data(train_data['data'], percentage=sample_percentage)
dev_examples = prepare_data(dev_data['data'])

!pip install accelerate>=0.20.1

import torch
from torch.nn import CrossEntropyLoss
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from datasets import load_dataset
import json

# Load GPT-2 model and tokenizer


# Simple question-answering dataset class

import torch
from torch.nn import CrossEntropyLoss
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import json
import random

# Load GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set the padding token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token

# Modify GPT-2 model for question-answering
model.config.is_decoder = True
model.config.add_cross_attention = True

# Add a linear layer for question-answering output
model.resize_token_embeddings(len(tokenizer))
model.config.num_labels = 2  # binary classification (answer or not)
model.classifier = torch.nn.Linear(model.config.hidden_size, model.config.num_labels)

# Load and preprocess the dataset
with open("/content/train-v2.0.json") as f:
    squad_data = json.load(f)

# Use only 10% of the data
sample_percentage = 0.1
squad_data_sampled = random.sample(squad_data['data'], int(len(squad_data['data']) * sample_percentage))

context_list = []
question_list = []
answer_list = []

for article in tqdm(squad_data_sampled, desc="Processing sampled data"):
    for paragraph in article['paragraphs']:
        context = paragraph['context']
        for qa in paragraph['qas']:
            question = qa['question']
            is_impossible = qa.get('is_impossible', False)
            if not is_impossible:
                answer_text = qa['answers'][0]['text']
            else:
                answer_text = ""

            context_list.append(context)
            question_list.append(question)
            answer_list.append(answer_text)

# Simple question-answering dataset class
class QADataset(Dataset):
    def __init__(self, contexts, questions, answers, max_length):
        self.contexts = contexts
        self.questions = questions
        self.answers = answers
        self.max_length = max_length

    def __len__(self):
        return len(self.contexts)

    def __getitem__(self, idx):
        input_ids_dict = tokenizer(
            self.contexts[idx] + " " + self.questions[idx],
            return_tensors='pt',
            padding='max_length',
            max_length=self.max_length,
            truncation=True
        )

        input_ids = input_ids_dict['input_ids'].squeeze()

        # Check if the end-of-sequence token is in the input_ids list
        eos_token_id = tokenizer.eos_token_id
        try:
            start_positions = input_ids.tolist().index(eos_token_id)
        except ValueError:
            # Handle the case where the end-of-sequence token is not present
            # You may choose to truncate the sequence or handle it differently
            print(f"End-of-sequence token ({eos_token_id}) not found in input_ids. Truncating sequence.")
            start_positions = min(len(input_ids), self.max_length - 1)

        attention_mask = torch.tensor([1] * start_positions + [0] * (self.max_length - start_positions))

        # Generate labels based on the presence of each token in the answer
        labels = torch.zeros(self.max_length, dtype=torch.long)
        end_positions = start_positions + len(self.answers[idx])

        if start_positions < self.max_length and end_positions < self.max_length:
            labels[start_positions:end_positions] = 1

        return {
            'input_ids': input_ids,
            'labels': labels,
            'attention_mask': attention_mask
        }

# Set the maximum sequence length
MAX_LENGTH = 256

# Create QADataset
train_dataset = QADataset(context_list, question_list, answer_list, MAX_LENGTH)

# DataLoader for the training dataset
BATCH_SIZE = 2
train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# Define hyperparameters
EPOCHS = 1
LEARNING_RATE = 0.01

# Define optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_data_loader) * EPOCHS)

# Define loss function
loss_fn = CrossEntropyLoss()

# Fine-tuning loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in tqdm(train_data_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}", unit="batch"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass for soft prompt embeddings
        outputs_soft_prompt = model.get_input_embeddings()(input_ids)
        loss_soft_prompt = torch.sum(outputs_soft_prompt)

        # Backpropagation for soft prompt embeddings
        loss_soft_prompt.backward()

        # Gradient clipping for soft prompt embeddings
        torch.nn.utils.clip_grad_norm_(model.get_input_embeddings().parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        # Continue with GPT-2 parameters (encoder and classifier)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss_gpt2 = outputs.loss

        total_loss += loss_gpt2.item()

        # Backpropagation for GPT-2 parameters
        loss_gpt2.backward()

        # Gradient clipping for GPT-2 parameters
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    average_loss = total_loss / len(train_data_loader)
    print(f"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {average_loss}")

# Save the fine-tuned model
model.save_pretrained("/content/fine_tuned_gpt2_qa")
tokenizer.save_pretrained("/content/fine_tuned_gpt2_qa")

# Load the fine-tuned model and tokenizer
fine_tuned_model = GPT2LMHeadModel.from_pretrained("/content/fine_tuned_gpt2_qa")
fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained("/content/fine_tuned_gpt2_qa")

# Function to generate answers
def generate_answer(question, context, max_length=256):
    input_text = context + " " + question

    input_ids = fine_tuned_tokenizer.encode(input_text, return_tensors="pt", max_length=max_length, truncation=True)
    output_ids = fine_tuned_model.generate(input_ids, max_length=max_length, num_beams=5, no_repeat_ngram_size=2)

    generated_answer = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return generated_answer

# Test the model with a sample question and context
sample_question = "What is the capital of France?"
sample_context = "France is a beautiful country located in Western Europe."

generated_answer = generate_answer(sample_question, sample_context)
print("Generated Answer:", generated_answer)

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from torch.nn import CrossEntropyLoss
from tqdm import tqdm
import json


class GPT2WithSoftPrompt(torch.nn.Module):
    def __init__(self, model_name, num_prompts, embedding_size=768):
        super(GPT2WithSoftPrompt, self).__init__()
        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)
        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)

    def forward(self, input_ids, prompt_ids):
        prompt_embeddings = self.soft_prompt(prompt_ids)
        base_embeddings = self.gpt2.transformer.wte(input_ids)
        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)
        outputs = self.gpt2(inputs_embeds=embeddings)
        return outputs


class DataLoaderAndPreprocessor:
    def __init__(self, json_file, num_prompts, max_len=512):
        self.json_file = json_file
        self.num_prompts = num_prompts
        self.max_len = max_len

    def load_data_from_json(self):
        with open(self.json_file, "r", encoding="utf-8") as file:
            data = json.load(file)

        context_list = [dp["context"] for item in data["data"] for dp in item["paragraphs"]]
        question_list = [dp["qas"][0]["question"] for item in data["data"] for dp in item["paragraphs"]]
        answer_list = [
            dp["qas"][0]["answers"][0]["text"] if dp["qas"] and dp["qas"][0]["answers"] else "" for item in data["data"]
            for dp in item["paragraphs"]
        ]

        return context_list[:50], question_list[:50], answer_list[:50]

    def preprocess_data(self):
        context_list, question_list, answer_list = self.load_data_from_json()

        # Perform preprocessing on the data
        tokenized_question = []
        tokenized_answer = []

        tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)

        for context, question, answer in zip(context_list, question_list, answer_list):
            # Tokenize context, question, and answer using the GPT-2 tokenizer
            context_tokens = tokenizer.encode(context, truncation=True, max_length=self.max_len)
            question_tokens = tokenizer.encode(question, truncation=True, max_length=self.max_len)
            answer_tokens = tokenizer.encode(answer, truncation=True, max_length=self.max_len)

            padded_article = question_tokens + [tokenizer.eos_token_id] * (self.max_len - len(question_tokens))
            padded_summary = answer_tokens + [tokenizer.eos_token_id] * (self.max_len + 4 - len(answer_tokens))

            tokenized_question.append(padded_article)
            tokenized_answer.append(padded_summary)

        return tokenized_question, tokenized_answer


class Trainer:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def evaluate(self, tokenized_articles, tokenized_summaries, prompt_id):
        # Your evaluation logic here
        model.eval()
        total_loss = 0
        total_matched = 0
        total_matched_ct = 0

        with torch.no_grad():
            for article, summary in tqdm(zip(tokenized_articles, tokenized_summaries), total=len(tokenized_articles),
                                         desc="Evaluation", unit="batch"):
                input_ids = torch.tensor(article).to(self.device)
                labels = torch.tensor(summary).to(self.device)
                outputs = self.model(input_ids, prompt_id)

                ignore_index = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else -100
                loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)
                total_loss += loss.item()

                # Metrics
                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())
                set2 = set(labels.cpu().numpy())

                # Calculate the intersection of sets
                intersection = set1.intersection(set2)

                # Calculate the percentage of indices in the first tensor that are also in the second tensor
                percentage = (len(intersection) / len(set1)) * 100
                total_matched += percentage
                total_matched_ct += 1

        avg_loss = total_loss / len(tokenized_summaries)
        avg_percentage_matched = total_matched / total_matched_ct

        print("Evaluation Loss:", avg_loss)
        print("Evaluation % Exact Match:", avg_percentage_matched)


# Constants
MODEL_NAME = "gpt2"
BATCH_SIZE = 1
EPOCHS = 1
PROMPT_TOKEN = "Answer the Following Question"
MAX_LEN = 512

# Soft Prompt Vocabulary
soft_prompt_vocab = ["Answer", "the", "Following", "Question"]
soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}
num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])
prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])

# Load and preprocess the data
data_loader = DataLoaderAndPreprocessor("train-v2.0.json", num_prompts)
tokenized_articles_train, tokenized_summaries_train = data_loader.preprocess_data()

data_loader_validation = DataLoaderAndPreprocessor("dev-v2.0.json", num_prompts)
tokenized_articles_validation, tokenized_summaries_validation = data_loader_validation.preprocess_data()

# Model Initialization
model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to("cpu")

# Trainer Initialization
trainer = Trainer(model, GPT2Tokenizer.from_pretrained(MODEL_NAME), device="cpu")

# Evaluation
trainer.evaluate(tokenized_articles_validation, tokenized_summaries_validation, prompt_id)

"""**Machine translation**"""

import torch
from torch.nn import CrossEntropyLoss
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import json
import random

# Load GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set the padding token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token

# Modify GPT-2 model for machine translation
model.config.is_decoder = True
model.resize_token_embeddings(len(tokenizer))

# Load and preprocess the English dataset
with open("/content/europarl-v7.de-en.en") as f:
    english_data = f.readlines()

# Load and preprocess the German dataset
with open("/content/europarl-v7.de-en.de") as f:
    german_data = f.readlines()

# Use only 10,000 entries from each dataset
sample_size = 10000
english_data_sampled = random.sample(english_data, min(sample_size, len(english_data)))
german_data_sampled = random.sample(german_data, min(sample_size, len(german_data)))

# Simple machine translation dataset class
class TranslationDataset(Dataset):
    def __init__(self, source_sentences, target_sentences, max_length, soft_prompt_token):
        self.source_sentences = source_sentences
        self.target_sentences = target_sentences
        self.max_length = max_length
        self.soft_prompt_token = soft_prompt_token

    def __len__(self):
        return min(len(self.source_sentences), len(self.target_sentences))

    def __getitem__(self, idx):
        if idx >= len(self.source_sentences) or idx >= len(self.target_sentences):
            # Handle the case where the index is out of range
            return None

        input_text = self.source_sentences[idx]
        target_text = self.target_sentences[idx]

        # Prepend soft prompt tokens to input
        input_text_with_prompt = self.soft_prompt_token + " " + input_text

        input_ids_dict = tokenizer(
            input_text_with_prompt,
            return_tensors='pt',
            padding='max_length',
            max_length=self.max_length,
            truncation=True
        )

        input_ids = input_ids_dict['input_ids'].squeeze()

        attention_mask = torch.tensor([1] * len(input_ids) + [0] * (self.max_length - len(input_ids)))

        labels = tokenizer(target_text, return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)['input_ids'].squeeze()

        return {
            'input_ids': input_ids,
            'labels': labels,
            'attention_mask': attention_mask
        }

# Set the maximum sequence length
MAX_LENGTH = 256

# Define soft prompt token
SOFT_PROMPT_TOKEN = "<soft_prompt>"

# Create TranslationDataset
translation_dataset = TranslationDataset(english_data_sampled, german_data_sampled, MAX_LENGTH, SOFT_PROMPT_TOKEN)

# DataLoader for the training dataset
BATCH_SIZE = 2
translation_data_loader = DataLoader(translation_dataset, batch_size=BATCH_SIZE, shuffle=True)

# Define hyperparameters
EPOCHS = 1
LEARNING_RATE = 0.01
LEARNING_RATE_SOFT_PROMPT = 0.001  # Adjust as needed

# Define optimizer and scheduler for GPT-2 parameters
optimizer_gpt2 = AdamW(model.parameters(), lr=LEARNING_RATE)
scheduler_gpt2 = get_linear_schedule_with_warmup(optimizer_gpt2, num_warmup_steps=0, num_training_steps=len(translation_data_loader) * EPOCHS)

# Define optimizer and scheduler for soft prompt embeddings
optimizer_soft_prompt = AdamW(model.get_input_embeddings().parameters(), lr=LEARNING_RATE_SOFT_PROMPT)
scheduler_soft_prompt = get_linear_schedule_with_warmup(optimizer_soft_prompt, num_warmup_steps=0, num_training_steps=len(translation_data_loader) * EPOCHS)

# Define loss function
loss_fn = CrossEntropyLoss()

# Fine-tuning loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in tqdm(translation_data_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}", unit="batch"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass for GPT-2 parameters
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss_gpt2 = outputs.loss

        total_loss += loss_gpt2.item()

        # Backpropagation for GPT-2 parameters
        loss_gpt2.backward()

        # Gradient clipping for GPT-2 parameters
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer_gpt2.step()
        scheduler_gpt2.step()
        optimizer_gpt2.zero_grad()

        # Forward pass for soft prompt embeddings
        outputs_soft_prompt = model.get_input_embeddings()(input_ids)
        loss_soft_prompt = torch.sum(outputs_soft_prompt)

        # Backpropagation for soft prompt embeddings
        loss_soft_prompt.backward()

        # Gradient clipping for soft prompt embeddings
        torch.nn.utils.clip_grad_norm_(model.get_input_embeddings().parameters(), 1.0)

        optimizer_soft_prompt.step()
        scheduler_soft_prompt.step()
        optimizer_soft_prompt.zero_grad()

    average_loss = total_loss / len(translation_data_loader)
    print(f"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {average_loss}")

# Save the fine-tuned model
model.save_pretrained("/content/fine_tuned_gpt2_translation_soft_prompt")
tokenizer.save_pretrained("/content/fine_tuned_gpt2_translation_soft_prompt")

# Function to translate an English sentence to German
def translate_to_german(english_sentence):
    # Prepend soft prompt token to input
    input_text_with_prompt = SOFT_PROMPT_TOKEN + " " + english_sentence

    # Tokenize and encode the input sentence
    input_ids = fine_tuned_tokenizer(input_text_with_prompt, return_tensors='pt')['input_ids']

    # Generate the German translation
    translated_output = fine_tuned_model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5, no_repeat_ngram_size=2, early_stopping=True, pad_token_id=fine_tuned_tokenizer.eos_token_id)

    # Decode and return the translated text, excluding the soft prompt token
    translated_text = fine_tuned_tokenizer.decode(translated_output[0], skip_special_tokens=True)
    return translated_text

# Test the translation
english_sentence_to_translate = "Hello, how are you?"
german_translation = translate_to_german(english_sentence_to_translate)

# Print the results
print("English Sentence:", english_sentence_to_translate)
print("German Translation:", german_translation)

import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from torch.nn import CrossEntropyLoss
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu
import json


class GPT2WithSoftPrompt(torch.nn.Module):
    def __init__(self, model_name, num_prompts, embedding_size=768):
        super(GPT2WithSoftPrompt, self).__init__()
        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)
        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)

    def forward(self, input_ids, prompt_ids):
        prompt_embeddings = self.soft_prompt(prompt_ids)
        base_embeddings = self.gpt2.transformer.wte(input_ids)
        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)
        outputs = self.gpt2(inputs_embeds=embeddings)
        return outputs


class DataLoaderAndPreprocessor:
    def __init__(self, english_file, german_file, num_prompts, max_len=500):
        self.english_file = english_file
        self.german_file = german_file
        self.num_prompts = num_prompts
        self.max_len = max_len

    def load_data_from_files(self):
        with open(self.english_file, "r", encoding="utf-8") as eng_file:
            english_list = eng_file.readlines()

        with open(self.german_file, "r", encoding="utf-8") as ger_file:
            german_list = ger_file.readlines()

        return english_list[:1500], german_list[:150]

    def preprocess_data(self):
        english_list, german_list = self.load_data_from_files()

        # Perform preprocessing on the data
        tokenized_english = []
        tokenized_german = []

        tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)

        for english_sentence, german_sentence in zip(english_list, german_list):
            english_tokens = tokenizer.encode(english_sentence, truncation=True, max_length=self.max_len)
            german_tokens = tokenizer.encode(german_sentence, truncation=True, max_length=self.max_len)

            # Pad the sequences to MAX_LEN
            padded_english = english_tokens + [tokenizer.eos_token_id] * (self.max_len - len(english_tokens))
            padded_german = german_tokens + [tokenizer.eos_token_id] * (self.max_len + 9 - len(german_tokens))

            tokenized_english.append(padded_english)
            tokenized_german.append(padded_german)

        return tokenized_english, tokenized_german


class Trainer:
    def __init__(self, model, tokenizer, device, optimizer, scheduler=None):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.optimizer = optimizer
        self.scheduler = scheduler

    def train(self, tokenized_articles, tokenized_summaries, prompt_id):
        # Your training logic here
        model.train()
        total_train_loss = 0

        for article, summary in tqdm(
            zip(tokenized_articles, tokenized_summaries), total=len(tokenized_articles), desc="Training", unit="batch"
        ):
            input_ids = torch.tensor(article).to(self.device)
            labels = torch.tensor(summary).to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(input_ids, prompt_id)

            ignore_index = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else -100
            train_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)
            total_train_loss += train_loss.item()

            train_loss.backward()
            self.optimizer.step()

            if self.scheduler:
                self.scheduler.step()

        avg_train_loss = total_train_loss / len(tokenized_summaries)
        print("Train Loss : ", avg_train_loss)

    def evaluate(self, tokenized_articles, tokenized_summaries, prompt_id):
        # Your evaluation logic here
        model.eval()
        test_pred_sentences = []
        test_true_sentences = []
        total_test_loss = 0
        test_percentage_matched = 0
        test_percentage_matched_ct = 0

        with torch.no_grad():
            for article, summary in tqdm(
                zip(tokenized_articles, tokenized_summaries), total=len(tokenized_articles), desc="Validation",
                unit="batch"
            ):
                input_ids = torch.tensor(article).to(self.device)
                labels = torch.tensor(summary).to(self.device)
                outputs = self.model(input_ids, prompt_id)

                # Bleu Score
                pred_logits = outputs.logits
                predicted_token_ids = torch.argmax(pred_logits, dim=-1)
                predicted_tokens = self.tokenizer.decode(predicted_token_ids, skip_special_tokens=True)
                test_pred_sentences.append(predicted_tokens.split())
                predicted_tokens = self.tokenizer.decode(labels, skip_special_tokens=True)
                test_true_sentences.append(predicted_tokens.split())

                ignore_index = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else -100
                test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)
                total_test_loss += test_loss.item()

                # Metrics
                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())
                set2 = set(labels.cpu().numpy())

                # Calculate the intersection of sets
                intersection = set1.intersection(set2)

                # Calculate the percentage of indices in the first tensor that are also in the second tensor
                percentage = (len(intersection) / len(set1)) * 100
                test_percentage_matched += percentage
                test_percentage_matched_ct += 1

        print("Test : % Exact Match: ", test_percentage_matched / test_percentage_matched_ct)
        avg_test_loss = total_test_loss / len(tokenized_summaries)
        print("Test Loss : ", avg_test_loss)
        bleu_score = corpus_bleu(test_true_sentences, test_pred_sentences)
        print(f'Test BLEU Score: {bleu_score}')


# Constants
MODEL_NAME = "gpt2"
BATCH_SIZE = 1
EPOCHS = 1
PROMPT_TOKEN = "Translate the following sentence from english to german :"
MAX_LEN = 500

# Soft Prompt Vocabulary
soft_prompt_vocab = ["Translate", "the", "following", "sentence", "from", "english", "to", "german", ":"]
soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}
num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])
prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])

# Load and preprocess the data
data_loader = DataLoaderAndPreprocessor("europarl-v7.de-en.en", "europarl-v7.de-en.de", num_prompts)
tokenized_articles_total, tokenized_summaries_total = data_loader.preprocess_data()
total_samples = len(tokenized_articles_total)
split_ratio = [0.8, 0.1, 0.1]

# Calculate the sizes of the three sets
train_size = int(total_samples * split_ratio[0])
val_size = int(total_samples * split_ratio[1])
test_size = int(total_samples * split_ratio[2])

# Split the data
tokenized_articles_train = tokenized_articles_total[:train_size]
tokenized_summaries_train = tokenized_summaries_total[:train_size]

tokenized_articles_validation = tokenized_articles_total[train_size:train_size + val_size]
tokenized_summaries_validation = tokenized_summaries_total[train_size:train_size + val_size]

tokenized_articles_test = tokenized_articles_total[train_size + val_size:]
tokenized_summaries_test = tokenized_summaries_total[train_size + val_size:]

device = "cpu"

# Model Initialization
model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)

# Trainer Initialization
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)
trainer = Trainer(model, GPT2Tokenizer.from_pretrained(MODEL_NAME), device="cpu", optimizer=optimizer, scheduler=scheduler)

# Training
for epoch in range(EPOCHS):
    print(f"Epoch {epoch + 1}/{EPOCHS}")
    trainer.train(tokenized_articles_train, tokenized_summaries_train, prompt_id)

# Evaluation
trainer.evaluate(tokenized_articles_test, tokenized_summaries_test, prompt_id)